{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4UjyaRAQrse"
      },
      "source": [
        "# Multilayer Perceptron\n",
        "\n",
        "\n",
        "Revisit classifying Iris flowers from the dataset in this notebook and compare an MLP with one hidden layer against the softmax classifier. Recall that the dataset contains 150 examples of Iris flowers belonging to 3 species *Iris-setosa*, *Iris-versicolor* and, *Iris-virginica*. Each example has 4 features *sepal length*, *sepal width*, *petal length*, and *petal width*. See the image below for an illustration.\n",
        "\n",
        "Use 90 samples (30 per class) for trainig the model and evaluate the model on the remaining 60 samples (20 per class).\n",
        "\n",
        "\n",
        "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png\" alt=\"alt text\" width=\"500\" height=\"200\">\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMVZo2rSQrsf"
      },
      "source": [
        "The data file contains four arrays, namely\n",
        "- **trnX:** A $90 \\times 4$ matrix storing training observations (each row is one training sample)\n",
        "- **trn_labels:** An array of size 90 storing the associated labels for each row of **trnX**\n",
        "- **tstX:** A $60 \\times 4$ matrix storing test samples (each row is one sample)\n",
        "- **tst_labels:** An array of size 60 storing the associated labels for each row of **tstX**\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl8i0xwGQrsf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XohUbAXQrsg"
      },
      "outputs": [],
      "source": [
        "npzfile = np.load('multilayer_perceptron.npz')\n",
        "trnX = npzfile['trnX']\n",
        "tstX = npzfile['tstX']\n",
        "trn_labels = npzfile['trn_labels']\n",
        "tst_labels = npzfile['tst_labels']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjMjU2rXQrsg"
      },
      "source": [
        "For a softmax classifier, the labels need to be converted to one-hot coded vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ux9wxYdHQrsg"
      },
      "outputs": [],
      "source": [
        "def create_one_hot(labels):\n",
        "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
        "    num_samples = labels.size\n",
        "    num_classes = np.unique(labels).size\n",
        "    labels_one_hot = np.zeros((num_samples, num_classes))\n",
        "    labels_one_hot[np.arange(num_samples),labels-labels.min()] = 1\n",
        "    return labels_one_hot\n",
        "\n",
        "num_samples, num_features = trnX.shape\n",
        "num_classes = np.unique(trn_labels).size\n",
        "\"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
        "trnY = create_one_hot(trn_labels)\n",
        "tstY = create_one_hot(tst_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8r1At3kQrsg"
      },
      "source": [
        "Recall that the softmax classifier has the form\n",
        "\\begin{align}\n",
        "p = \\mathrm{softmax}\\big({z}\\big) &=\n",
        "\\mathrm{softmax}\\big({W}^\\top {x} \\big)\\\\\n",
        "\\mathrm{softmax}\\big({z_1,z_2,\\cdots,z_K}\\big) &= \\frac{1}{\\sum_i \\exp(z_i)}\n",
        "\\big(\\exp(z_1),\\exp(z_2),\\cdots,\\exp(z_K)\\big)^\\top\n",
        "\\end{align}\n",
        "\n",
        "The code below realizes the softmax classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceHq9uW6Qrsg"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    z1 = np.exp(x)\n",
        "    z = z1.T/np.sum(z1,axis=1)\n",
        "    return(z.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilsla-iZQrsg"
      },
      "source": [
        "The CE loss (NLL of softmax output) is:\n",
        "\n",
        "\\begin{align}\n",
        "    \\mathcal{L}_{\\mathrm{NLL}}\\big({W}\\big) \\triangleq -\n",
        "    \\sum_{i=1}^m {y}_i^\\top \\log\\big(\\hat{y}_i\\big)\\;.   \n",
        "\\end{align}\n",
        "with $\\hat{y}_i = \\mathrm{softmax}\\big({W}^\\top {x}_i \\big)$.\n",
        "The function below implements the NLL loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDiw0eDeQrsh"
      },
      "outputs": [],
      "source": [
        "def nll_loss(y,y_hat):\n",
        "\n",
        "    # Avoid division by zero by adding a small epsilon\n",
        "    epsilon = 1e-15\n",
        "\n",
        "    # Ensure y_hat is not exactly 0 or 1 to prevent log(0) or log(1) issues\n",
        "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon) # Clip the values in an array\n",
        "\n",
        "    # Calculate the CE loss\n",
        "    ce_loss = -np.sum(y * np.log(y_hat))\n",
        "    return ce_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGTHrjRtQrsh",
        "outputId": "876a2a22-7006-4d33-cbf2-91bda1631cef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Softmax - iter:  1 :  loss=111.027, test accuracy= 33.33\n",
            "Softmax - iter: 11 :  loss=104.827, test accuracy= 35.00\n",
            "Softmax - iter: 21 :  loss=99.951, test accuracy= 43.33\n",
            "Softmax - iter: 31 :  loss=96.153, test accuracy= 63.33\n",
            "Softmax - iter: 41 :  loss=93.220, test accuracy= 65.00\n",
            "Softmax - iter: 51 :  loss=90.964, test accuracy= 65.00\n",
            "Softmax - iter: 61 :  loss=89.225, test accuracy= 65.00\n",
            "Softmax - iter: 71 :  loss=87.874, test accuracy= 66.67\n",
            "Softmax - iter: 81 :  loss=86.804, test accuracy= 66.67\n",
            "Softmax - iter: 91 :  loss=85.939, test accuracy= 80.00\n",
            "Softmax - iter:101 :  loss=85.218, test accuracy= 91.67\n",
            "Softmax - iter:111 :  loss=84.601, test accuracy= 93.33\n",
            "Softmax - iter:121 :  loss=84.057, test accuracy= 78.33\n",
            "Softmax - iter:131 :  loss=83.565, test accuracy= 68.33\n",
            "Softmax - iter:141 :  loss=83.110, test accuracy= 66.67\n",
            "Softmax - iter:151 :  loss=82.682, test accuracy= 66.67\n",
            "Softmax - iter:161 :  loss=82.275, test accuracy= 66.67\n",
            "Softmax - iter:171 :  loss=81.882, test accuracy= 66.67\n",
            "Softmax - iter:181 :  loss=81.502, test accuracy= 66.67\n",
            "Softmax - iter:191 :  loss=81.131, test accuracy= 66.67\n",
            "Softmax - iter:201 :  loss=80.768, test accuracy= 66.67\n",
            "Softmax - iter:211 :  loss=80.411, test accuracy= 66.67\n",
            "Softmax - iter:221 :  loss=80.061, test accuracy= 66.67\n",
            "Softmax - iter:231 :  loss=79.716, test accuracy= 66.67\n",
            "Softmax - iter:241 :  loss=79.376, test accuracy= 66.67\n",
            "Softmax - iter:251 :  loss=79.040, test accuracy= 66.67\n",
            "Softmax - iter:261 :  loss=78.709, test accuracy= 66.67\n",
            "Softmax - iter:271 :  loss=78.382, test accuracy= 66.67\n",
            "Softmax - iter:281 :  loss=78.060, test accuracy= 66.67\n",
            "Softmax - iter:291 :  loss=77.741, test accuracy= 66.67\n",
            "Softmax - iter:301 :  loss=77.426, test accuracy= 66.67\n",
            "Softmax - iter:311 :  loss=77.115, test accuracy= 66.67\n",
            "Softmax - iter:321 :  loss=76.808, test accuracy= 66.67\n",
            "Softmax - iter:331 :  loss=76.505, test accuracy= 66.67\n",
            "Softmax - iter:341 :  loss=76.205, test accuracy= 66.67\n",
            "Softmax - iter:351 :  loss=75.908, test accuracy= 66.67\n",
            "Softmax - iter:361 :  loss=75.615, test accuracy= 66.67\n",
            "Softmax - iter:371 :  loss=75.326, test accuracy= 66.67\n",
            "Softmax - iter:381 :  loss=75.040, test accuracy= 66.67\n",
            "Softmax - iter:391 :  loss=74.757, test accuracy= 66.67\n",
            "Softmax - iter:401 :  loss=74.478, test accuracy= 66.67\n",
            "Softmax - iter:411 :  loss=74.202, test accuracy= 66.67\n",
            "Softmax - iter:421 :  loss=73.929, test accuracy= 66.67\n",
            "Softmax - iter:431 :  loss=73.659, test accuracy= 66.67\n",
            "Softmax - iter:441 :  loss=73.392, test accuracy= 66.67\n",
            "Softmax - iter:451 :  loss=73.128, test accuracy= 66.67\n",
            "Softmax - iter:461 :  loss=72.868, test accuracy= 66.67\n",
            "Softmax - iter:471 :  loss=72.610, test accuracy= 66.67\n",
            "Softmax - iter:481 :  loss=72.355, test accuracy= 66.67\n",
            "Softmax - iter:491 :  loss=72.103, test accuracy= 66.67\n"
          ]
        }
      ],
      "source": [
        "# Softmax\n",
        "W = np.random.randn(num_features,num_classes)\n",
        "\n",
        "lr = 0.001\n",
        "max_iter = 500\n",
        "loss_softmax = []\n",
        "\n",
        "for iter in range(max_iter):\n",
        "    h = np.matmul(trnX,W)\n",
        "    y_hat = softmax(h)\n",
        "    tmp_loss = nll_loss(trnY,y_hat)\n",
        "    loss_softmax.append(tmp_loss)\n",
        "    grad = np.matmul(trnX.T,(y_hat-trnY))\n",
        "    W -= lr * grad\n",
        "\n",
        "    # computing accuracy\n",
        "    if np.mod(iter, 10) == 1:\n",
        "        h = np.matmul(tstX,W)\n",
        "        y_hat = softmax(h)\n",
        "        pred_label = np.argmax(y_hat, axis=1)\n",
        "        acc_iter = np.mean(pred_label==tst_labels)\n",
        "        print(f'Softmax - iter:{iter:3} :  loss={tmp_loss:.3f}, test accuracy= {100*acc_iter:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8T_YkfqQrsh"
      },
      "source": [
        "## Multilayer Perceptron\n",
        "\n",
        "Now try to perform the following\n",
        "\n",
        "* Can you add a bias to the model?\n",
        "\n",
        "* The learning rate is an important parameter of the model. increase it and study the behavior of the algorithm\n",
        "\n",
        "* The direct implementation of the softmax function is prone to neumerical instabilities. Implement a robust softmax function using the invariance property of the softmax.\n",
        "\n",
        "* Related to the above, implement a robust loss using the log-sum trick"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yyB8nsmLT0VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS2Tpp_EQrsh"
      },
      "source": [
        "As for the activation function, a ReLU function is defined as\n",
        "\n",
        "$\\mathrm{ReLU}(x) = \\max(x,0)$ (see below)\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*DfMRHwxY1gyyDmrIAd-gjQ.png\" alt=\"alt text\" width=\"400\" height=\"200\">\n",
        "\n",
        "The code below implements the ReLU function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r2MSiMuQrsh"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return(np.maximum(x, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rNQJ-IYQrsi"
      },
      "source": [
        "The MLP model has the following structure:\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*4_BDTvgB6WoYVXyxO8lDGA.png\" alt=\"alt text\" width=\"400\" height=\"350\">\n",
        "\n",
        "\n",
        "\n",
        "The following equations provide us with the gradient information to perform GD\n",
        "\n",
        "\\begin{align}\n",
        "\\nabla_{W_2}\\mathcal{L} &= h_1\\Big(\\nabla_{h_2}\\mathcal{L}\\Big)^\\top\\\\\n",
        "\\nabla_{W_1}\\mathcal{L} &= x\\Big(\\nabla_{z_1}\\mathcal{L}\\Big)^\\top\\\\\n",
        "\\end{align}\n",
        "with\n",
        "\\begin{align}\n",
        "\\nabla_{h_2}\\mathcal{L} &= \\hat{y} - y\\\\\n",
        "\\nabla_{h_1}\\mathcal{L} &= W_2\\nabla_{h_2}\\mathcal{L}\\\\\n",
        "\\nabla_{z_1}\\mathcal{L} &= \\nabla_{h_1}\\mathcal{L}\\odot\\mathrm{sign}(h_1)\\\\\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPgGGsnwQrsi",
        "outputId": "13845e5b-e352-4501-a7c2-ec3af02cb6ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP - iter:  1  loss=120.574, test accuracy=33.33\n",
            "MLP - iter: 11  loss=90.388, test accuracy=33.33\n",
            "MLP - iter: 21  loss=84.497, test accuracy=66.67\n",
            "MLP - iter: 31  loss=79.674, test accuracy=66.67\n",
            "MLP - iter: 41  loss=75.590, test accuracy=66.67\n",
            "MLP - iter: 51  loss=72.082, test accuracy=66.67\n",
            "MLP - iter: 61  loss=69.039, test accuracy=66.67\n",
            "MLP - iter: 71  loss=66.372, test accuracy=66.67\n",
            "MLP - iter: 81  loss=64.012, test accuracy=66.67\n",
            "MLP - iter: 91  loss=61.902, test accuracy=66.67\n",
            "MLP - iter:101  loss=59.999, test accuracy=66.67\n",
            "MLP - iter:111  loss=58.265, test accuracy=66.67\n",
            "MLP - iter:121  loss=56.673, test accuracy=66.67\n",
            "MLP - iter:131  loss=55.200, test accuracy=66.67\n",
            "MLP - iter:141  loss=53.825, test accuracy=66.67\n",
            "MLP - iter:151  loss=52.541, test accuracy=66.67\n",
            "MLP - iter:161  loss=51.327, test accuracy=66.67\n",
            "MLP - iter:171  loss=50.173, test accuracy=66.67\n",
            "MLP - iter:181  loss=49.070, test accuracy=66.67\n",
            "MLP - iter:191  loss=48.012, test accuracy=66.67\n",
            "MLP - iter:201  loss=46.991, test accuracy=66.67\n",
            "MLP - iter:211  loss=46.005, test accuracy=66.67\n",
            "MLP - iter:221  loss=45.047, test accuracy=66.67\n",
            "MLP - iter:231  loss=44.120, test accuracy=66.67\n",
            "MLP - iter:241  loss=43.215, test accuracy=66.67\n",
            "MLP - iter:251  loss=42.331, test accuracy=68.33\n",
            "MLP - iter:261  loss=41.466, test accuracy=70.00\n",
            "MLP - iter:271  loss=40.618, test accuracy=75.00\n",
            "MLP - iter:281  loss=39.787, test accuracy=78.33\n",
            "MLP - iter:291  loss=38.971, test accuracy=80.00\n",
            "MLP - iter:301  loss=38.170, test accuracy=80.00\n",
            "MLP - iter:311  loss=37.384, test accuracy=83.33\n",
            "MLP - iter:321  loss=36.615, test accuracy=83.33\n",
            "MLP - iter:331  loss=35.865, test accuracy=86.67\n",
            "MLP - iter:341  loss=35.135, test accuracy=88.33\n",
            "MLP - iter:351  loss=34.429, test accuracy=90.00\n",
            "MLP - iter:361  loss=33.753, test accuracy=90.00\n",
            "MLP - iter:371  loss=33.112, test accuracy=91.67\n",
            "MLP - iter:381  loss=32.511, test accuracy=95.00\n",
            "MLP - iter:391  loss=31.954, test accuracy=95.00\n",
            "MLP - iter:401  loss=31.441, test accuracy=96.67\n",
            "MLP - iter:411  loss=30.968, test accuracy=96.67\n",
            "MLP - iter:421  loss=30.528, test accuracy=96.67\n",
            "MLP - iter:431  loss=30.113, test accuracy=96.67\n",
            "MLP - iter:441  loss=29.714, test accuracy=96.67\n",
            "MLP - iter:451  loss=29.327, test accuracy=96.67\n",
            "MLP - iter:461  loss=28.949, test accuracy=96.67\n",
            "MLP - iter:471  loss=28.578, test accuracy=96.67\n",
            "MLP - iter:481  loss=28.215, test accuracy=96.67\n",
            "MLP - iter:491  loss=27.858, test accuracy=96.67\n"
          ]
        }
      ],
      "source": [
        "# MLP\n",
        "n0 = num_features #number of input features\n",
        "n1 = 100 #size of the hidden layer\n",
        "n2 = num_classes #number of classes\n",
        "W1 = np.random.rand(n0,n1)\n",
        "W2 = np.random.rand(n1,n2)\n",
        "\n",
        "lr = 0.001\n",
        "max_iter = 500\n",
        "loss_mlp = []\n",
        "for iter in range(max_iter):\n",
        "    #forward pass\n",
        "    z1 = np.matmul(trnX,W1)\n",
        "    h1 = relu(z1)\n",
        "    h2 = np.matmul(h1,W2)\n",
        "    y_hat = softmax(h2)\n",
        "\n",
        "    tmp_loss = nll_loss(trnY,y_hat)\n",
        "    loss_mlp.append(tmp_loss)\n",
        "\n",
        "    #backward pass\n",
        "    dldh2 = (y_hat - trnY)\n",
        "    dldW2 = np.matmul(h1.T,dldh2)\n",
        "\n",
        "\n",
        "    dldh1 = np.matmul(dldh2,W2.T)\n",
        "    dldz1 = dldh1*np.sign(z1)\n",
        "    dldW1 = np.matmul(trnX.T,dldz1)\n",
        "\n",
        "    # GD\n",
        "    W2 -= lr*dldW2\n",
        "    W1 -= lr*dldW1\n",
        "\n",
        "    if np.mod(iter, 10) == 1:\n",
        "        h1 = np.matmul(tstX,W1)\n",
        "        h2 = relu(h1)\n",
        "        h3 = np.matmul(h2,W2)\n",
        "        y_hat = softmax(h3)\n",
        "        pred_label = np.argmax(y_hat, axis=1)\n",
        "        acc_iter = np.mean(pred_label==tst_labels)\n",
        "        print(f'MLP - iter:{iter:3}  loss={tmp_loss:.3f}, test accuracy={100*acc_iter:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSp6ujzhQrsi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}