{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ-KqXX0acP7"
      },
      "source": [
        "# Softmax\n",
        "\n",
        "\n",
        "The flowers from the iris dataset will be classified in this notebook. The Iris dataset contains 150 examples of Iris flowers belonging to 3 species *Iris-setosa*, *Iris-versicolor* and, *Iris-virginica* equally (Each group has 50 samples). Each example has 4 features *sepal length*, *sepal width*, *petal length*, and *petal width*. See the image below for an illustration.\n",
        "\n",
        "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png\" alt=\"alt text\" width=\"500\" height=\"200\">\n",
        "\n",
        "In this notebook,  the problem of multiclass classification will be directly addressed using a softmax classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmV0IrnzacP8",
        "outputId": "ae1ce413-d45d-46be-a0b3-a195ea7a0d37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "       0    1    2    3               4\n",
            "0    5.1  3.5  1.4  0.2     Iris-setosa\n",
            "1    4.9  3.0  1.4  0.2     Iris-setosa\n",
            "2    4.7  3.2  1.3  0.2     Iris-setosa\n",
            "3    4.6  3.1  1.5  0.2     Iris-setosa\n",
            "4    5.0  3.6  1.4  0.2     Iris-setosa\n",
            "..   ...  ...  ...  ...             ...\n",
            "145  6.7  3.0  5.2  2.3  Iris-virginica\n",
            "146  6.3  2.5  5.0  1.9  Iris-virginica\n",
            "147  6.5  3.0  5.2  2.0  Iris-virginica\n",
            "148  6.2  3.4  5.4  2.3  Iris-virginica\n",
            "149  5.9  3.0  5.1  1.8  Iris-virginica\n",
            "\n",
            "[150 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "URL_='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
        "data = pd.read_csv(URL_, header = None)\n",
        "\n",
        "print(type(data))\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O5dwx0WacP9"
      },
      "source": [
        "Below, store the observations into a matrix of size $150 \\times 4$ (call it $\\boldsymbol{X}$), and give labels $\\{0,1,2\\}$ to *Iris-setosa*, *Iris-versicolor* and, *Iris-virginica*, respectively. This is obviously just a choice and label Iris-setosa by say 2.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9ydPKlEacP9"
      },
      "outputs": [],
      "source": [
        "X = np.asarray(data.iloc[:,:4]) # Use data.iloc[:,:4] to abondon the label column\n",
        "# Use np.asarray() to convert the data type to array\n",
        "num_samples, num_features = X.shape\n",
        "# print(X)\n",
        "\n",
        "num_classes = 3\n",
        "\n",
        "y = np.zeros((num_samples, num_classes), dtype='int')\n",
        "\n",
        "for i in range(y.shape[0]):\n",
        "    if i < 50:\n",
        "        y[i][0] = 1\n",
        "    elif i < 99:\n",
        "        y[i][1] = 1\n",
        "    else:\n",
        "        y[i][2] = 1\n",
        "\n",
        "# Create one-hot vectors for class labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKhgfJv8acP-"
      },
      "source": [
        "Recall that the softmax classifier has the form\n",
        "\\begin{align}\n",
        "p = \\mathrm{softmax}\\big({z}\\big) &=\n",
        "\\mathrm{softmax}\\big({W}^\\top {x} \\big)\\\\\n",
        "\\mathrm{softmax}\\big({z_1,z_2,\\cdots,z_K}\\big) &= \\frac{1}{\\sum_i \\exp(z_i)}\n",
        "\\big(\\exp(z_1),\\exp(z_2),\\cdots,\\exp(z_K)\\big)^\\top\n",
        "\\end{align}\n",
        "\n",
        "The code below realizes the softmax classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE12yuIhacP-"
      },
      "outputs": [],
      "source": [
        "def softmax_func(W, X):\n",
        "    z0 = np.matmul(X, W)\n",
        "    z1 = np.exp(z0)\n",
        "    p = z1.T / np.sum(z1, axis=1)\n",
        "\n",
        "    return(p.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zel_kVCRacP-"
      },
      "source": [
        "The CE loss (NLL of softmax output) is:\n",
        "\n",
        "\\begin{align}\n",
        "    \\mathcal{L}_{\\mathrm{NLL}}\\big({W}\\big) \\triangleq -\\frac{1}{m}\n",
        "    \\sum_{i=1}^m {y}_i^\\top \\log\\big(\\hat{y}_i\\big)\\;.   \n",
        "\\end{align}\n",
        "with $\\hat{y}_i = \\mathrm{softmax}\\big({W}^\\top {x}_i \\big)$.\n",
        "The function below implements the NLL loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ozKuyW1acP-"
      },
      "outputs": [],
      "source": [
        "def nll_loss(y, y_hat):\n",
        "\n",
        "    # Avoid division by zero by adding a small epsilon\n",
        "    epsilon = 1e-15\n",
        "\n",
        "    # Ensure y_hat is not exactly 0 or 1 to prevent log(0) or log(1) issues\n",
        "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon) # Clip the values in an array\n",
        "\n",
        "    # Calculate the CE loss\n",
        "    ce_loss = -np.sum(y * np.log(y_hat))\n",
        "\n",
        "    return ce_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tj3Q6vUacP-"
      },
      "source": [
        "To update the weights of the model ${W}$, the gradient of the NLL wrt ${W}$ is required which can be written as:\n",
        "\n",
        "\\begin{align}\n",
        "    \\nabla_{W}  \\mathcal{L}_{\\mathrm{NLL}} = \\frac{1}{m} \\sum_{i=1}^m x_i \\big(\\hat{y}_i - {y}_i\\big)^\\top\\;.\n",
        "\\end{align}\n",
        "\n",
        "To write a loop to perform gradient descent in order to learn $W$. The updating rule for $W$ can be written as:\n",
        "\\begin{align}\n",
        "W \\gets W - \\eta  \\nabla_{W}  \\mathcal{L}_{\\mathrm{NLL}}\n",
        "\\end{align}\n",
        "\n",
        "The parameter $\\eta \\in (0,1]$ is the learning rate of the algorithm (call it lR below). The training loop is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDkhAjxzacP_",
        "outputId": "c29e9dd4-f59c-4d66-b660-1174cfbcf950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter:  0 : loss value 1123.093, classification accuracy 33.33%\n",
            "iter:  1 : loss value 1058.168, classification accuracy 33.33%\n",
            "iter:  2 : loss value 1009.615, classification accuracy 27.33%\n",
            "iter:  3 : loss value 970.020, classification accuracy 25.33%\n",
            "iter:  4 : loss value 933.860, classification accuracy 19.33%\n",
            "iter:  5 : loss value 898.822, classification accuracy 22.00%\n",
            "iter:  6 : loss value 864.145, classification accuracy 24.00%\n",
            "iter:  7 : loss value 829.593, classification accuracy 24.67%\n",
            "iter:  8 : loss value 795.094, classification accuracy 27.33%\n",
            "iter:  9 : loss value 760.626, classification accuracy 28.00%\n",
            "iter: 10 : loss value 726.183, classification accuracy 33.33%\n",
            "iter: 11 : loss value 691.760, classification accuracy 34.67%\n",
            "iter: 12 : loss value 657.359, classification accuracy 35.33%\n",
            "iter: 13 : loss value 622.978, classification accuracy 36.67%\n",
            "iter: 14 : loss value 588.617, classification accuracy 36.67%\n",
            "iter: 15 : loss value 554.277, classification accuracy 39.33%\n",
            "iter: 16 : loss value 519.961, classification accuracy 40.00%\n",
            "iter: 17 : loss value 485.672, classification accuracy 41.33%\n",
            "iter: 18 : loss value 451.420, classification accuracy 41.33%\n",
            "iter: 19 : loss value 417.219, classification accuracy 43.33%\n",
            "iter: 20 : loss value 383.100, classification accuracy 44.00%\n",
            "iter: 21 : loss value 349.116, classification accuracy 46.00%\n",
            "iter: 22 : loss value 315.368, classification accuracy 47.33%\n",
            "iter: 23 : loss value 282.038, classification accuracy 48.00%\n",
            "iter: 24 : loss value 249.451, classification accuracy 48.00%\n",
            "iter: 25 : loss value 218.157, classification accuracy 48.67%\n",
            "iter: 26 : loss value 189.011, classification accuracy 51.33%\n",
            "iter: 27 : loss value 163.156, classification accuracy 52.67%\n",
            "iter: 28 : loss value 141.745, classification accuracy 54.00%\n",
            "iter: 29 : loss value 125.415, classification accuracy 61.33%\n",
            "iter: 30 : loss value 113.909, classification accuracy 72.00%\n",
            "iter: 31 : loss value 106.258, classification accuracy 83.33%\n",
            "iter: 32 : loss value 101.303, classification accuracy 90.00%\n",
            "iter: 33 : loss value 98.081, classification accuracy 92.00%\n",
            "iter: 34 : loss value 95.927, classification accuracy 92.67%\n",
            "iter: 35 : loss value 94.417, classification accuracy 91.33%\n",
            "iter: 36 : loss value 93.297, classification accuracy 92.00%\n",
            "iter: 37 : loss value 92.413, classification accuracy 90.67%\n",
            "iter: 38 : loss value 91.677, classification accuracy 91.33%\n",
            "iter: 39 : loss value 91.033, classification accuracy 92.00%\n",
            "iter: 40 : loss value 90.450, classification accuracy 90.67%\n",
            "iter: 41 : loss value 89.907, classification accuracy 90.00%\n",
            "iter: 42 : loss value 89.394, classification accuracy 89.33%\n",
            "iter: 43 : loss value 88.902, classification accuracy 90.00%\n",
            "iter: 44 : loss value 88.427, classification accuracy 90.00%\n",
            "iter: 45 : loss value 87.966, classification accuracy 90.00%\n",
            "iter: 46 : loss value 87.516, classification accuracy 90.00%\n",
            "iter: 47 : loss value 87.077, classification accuracy 90.00%\n",
            "iter: 48 : loss value 86.647, classification accuracy 90.00%\n",
            "iter: 49 : loss value 86.226, classification accuracy 90.00%\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(10)\n",
        "W = np.random.randn(num_features, num_classes) # Initialize the W parameter\n",
        "\n",
        "lr = 0.0001\n",
        "max_iter = 50\n",
        "loss = []\n",
        "\n",
        "# Run iterations by Gradient Descent\n",
        "for iter in range(max_iter):\n",
        "    y_hat = softmax_func(W, X)\n",
        "    loss_iter = nll_loss(y, y_hat)\n",
        "    loss.append(loss_iter)\n",
        "    grad = np.matmul(X.T, (y_hat-y))\n",
        "    W -= lr * grad\n",
        "\n",
        "    # Computing accuracy\n",
        "    pred_label = np.argmax(y_hat, axis=1) # Return indices of the max element of the array in a particular axis\n",
        "    acc_iter = np.mean(pred_label == np.argmax(y, axis=1))\n",
        "\n",
        "    print(f'iter:{iter:3} : loss value {loss_iter:.3f}, classification accuracy {100*acc_iter:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IULsNhZKacP_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}